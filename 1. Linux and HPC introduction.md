***Introduction to Linux and HPC for Conservation Genomics***

Welcome to the Linux and HPC tutorial for conservation genomics! In this session, we’ll focus on the essential skills needed to navigate the command line, manage files, and run analyses on a high-performance computing (HPC) cluster. This tutorial is designed for graduate students with varying levels of experience, so don’t worry if you’re new to Linux or HPC environments—we’ll take it step by step.

**Why Linux and HPC for Conservation Genomics?**

Whole-genome sequencing (WGS) generates large datasets that require powerful computational resources for analysis. HPC clusters provide the necessary infrastructure, but using them effectively requires familiarity with Linux, the command-line interface (CLI), and job scheduling systems.

**Section 1: Basics of the Linux Command Line**

The Linux terminal is your gateway to controlling the HPC environment. Here are the basics to get you started:

**Accessing the Terminal**

•	On Linux/Mac: Open the built-in terminal application.

•	On Windows: Use tools like PuTTY, MobaXterm, or Windows Subsystem for Linux (WSL).

•	Connect to the HPC cluster via SSH:

ssh your_username@hpc.server.address

**Navigation and File Management**

1.	**Navigating Directories**

•	pwd – Print your current directory:

pwd

•	ls – List files in the current directory:

ls

•	cd – Change directory:

cd /path/to/directory

•	Use cd .. to move up one directory.

2.	**Creating and Managing Files/Directories**

•	mkdir – Create a directory:

mkdir new_directory

•	touch – Create an empty file:

touch file.txt

•	cp – Copy files:

cp file.txt /destination/path/

•	mv – Move or rename files:

mv file.txt new_name.txt

•	rm – Remove files:

rm file.txt

•	Use rm -r to remove directories.

3.	**Viewing File Contents**

•	cat – View the entire file:

cat file.txt

•	less – View large files page by page:

less file.txt

•	head/tail – View the first/last lines of a file:

head file.txt

tail file.txt

4.	**Editing Files**

•	Use editors like nano, vi, or vim:

nano file.txt

**Section 2: Using an HPC Cluster**

HPC clusters are shared resources with multiple users and nodes for heavy computations. Here’s how to interact with them.

**Understanding HPC Components**

•	**Login Node**: The entry point where you manage files and submit jobs.

•	**Compute Node**: Where your jobs are executed.

•	**Scheduler**: Manages job queues (e.g., SLURM, PBS, or LSF).

**Transferring Files to/from the HPC**

1.	Use scp to copy files:

scp local_file.txt your_username@hpc.server.address:/path/to/destination/

2.	Use rsync for efficient transfers:

rsync -av local_directory/ your_username@hpc.server.address:/remote/directory/

**Job Submission with SLURM**

SLURM (Simple Linux Utility for Resource Management) is a common scheduler. You interact with SLURM using scripts and commands.

1.	**Writing a Job Script**

Example job script (job_script.sh):

#!/bin/bash

#SBATCH --job-name=genome_analysis

#SBATCH --output=output.log

#SBATCH --error=error.log

#SBATCH --time=24:00:00

#SBATCH --nodes=1

#SBATCH --ntasks=4

#SBATCH --mem=16G

module load bioinformatics_tool

tool_command input_file output_file

2.	**Submitting a Job**

Use sbatch to submit:

sbatch job_script.sh

3.	**Monitoring Jobs**

•	squeue – Check the status of jobs:

squeue -u your_username

•	scancel – Cancel a job:

scancel job_id

4.	**Checking Job Output**

After your job finishes, check the output files specified in the script (output.log, error.log).

**Section 3: Practical Exercises**

1.	**Navigation Challenge**:

Navigate to a specified directory, create a new directory, and copy files into it.

2.	**Simple Job Submission**:

Write a SLURM script that prints “Hello, HPC!” to an output file, submit it, and check the result.

3.	**Data Transfer**:

Transfer a dataset from your local machine to the HPC and verify the transfer.

4.	**Genome Analysis**:

Run a simple genome analysis command using a bioinformatics tool like bcftools or samtools.

**Section 4: Best Practices**

1.	**Use Descriptive Names**: For directories, files, and job scripts.

2.	**Check Resource Usage**: Avoid over-requesting memory or CPUs.

3.	**Automate Repetitive Tasks**: Use Bash loops or scripts.

4.	**Keep Logs**: Redirect output and error streams for troubleshooting.

5.	**Clean Up**: Remove intermediate files after use to save storage.

**Resources**

•	[Linux Command Cheat Sheet](https://www.linuxcommand.org/)

•	[SLURM Documentation](https://slurm.schedmd.com/documentation.html)

•	[Bioinformatics Tools Documentation](https://bioinformatics.org/tools/)

By the end of this tutorial, you should be comfortable navigating a Linux environment and submitting jobs to an HPC cluster. These skills are foundational for performing WGS analyses in conservation genomics. Let’s dive in!
