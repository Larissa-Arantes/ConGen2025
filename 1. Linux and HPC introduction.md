# Introduction to Linux and HPC for Conservation Genomics

Welcome to the Linux and HPC tutorial for conservation genomics! In this session, we’ll focus on the essential skills needed to navigate the command line, manage files, and run analyses on a high-performance computing (HPC) cluster.

**Why Linux and HPC for Conservation Genomics?

Whole-genome sequencing (WGS) generates large datasets that require powerful computational resources for analysis. HPC clusters provide the necessary infrastructure, but using them effectively requires familiarity with Linux, the command-line interface (CLI), and job scheduling systems.

## Section 1: Basics of the Linux Command Line

The Linux terminal is your gateway to controlling the HPC environment. Here are the basics to get you started:

### Logging into Purdue Anvil

Go to the website: https://ondemand.anvil.rcac.purdue.edu

Log in with your ACCESS login name and password

Navigate to Shell Access -> now you are on the command-line interface! 

## Navigation and File Management

This is called the shell, which is a computer program that allows you to control your computer using keyboard commands rather than using your mouse. Usually we only run commands like navigating directories, making a file, or deleting files on the command line. 

- Files are organized into directories, similar to folder structure on your laptop.
- The dollar sign is the "prompt", which you type commands on the command line to navigate the directory. We will get to running scripts later.
  
1.	**Navigating Directories**

•	pwd – Print your current directory:

```
pwd
```
When we log in, we are in our home directory: `/home/x-lhennelly`

We have other directories too. I will show the different directories on my computer and draw them on the board. 
- Scratch -> used to store large data files and output results from analyses.
- Home directory -> only used for storing you scripts, our out and error files, and small output files that are small in size
  
•	ls – List files in the current directory:

```
ls
```

•	mkdir – Create a directory:

```
mkdir 01_linux_introduction 02_sequencing_dataquality 03_readmapping_variantcalling 04_population_structure  05_genetic_diversity 06_runs_of_homozygosity 07_deleterious_variants 08_demographic_inference 09_phylogenomic_introgression

ls 
```
Now you should see the directories for different days of the course

•	cd – Change directory:

```
cd 01_linux_introduction
pwd
```
Now we are in the 01_linux_introduction directory

•	Use cd .. to move up one directory.
```
cd ..
```
We can also just use `cd` and that will place us into our home directory
```
cd 01_linux_introduction
cd
```

## Making and managing files

•	touch – Create an empty file:
```
cd 01_linux_introduction
touch file.txt
```

•	cp – Copy files:

```bash
cp file.txt /home/x-lhennelly
cd ..
ls
```

•	rm – Remove files:

```bash
rm /home/x-lhennelly/file.txt
```

3.	**Viewing File Contents**

•	cat – View the entire file:

```bash
less file.txt
```

•	head/tail – View the first/last lines of a file:

```bash
head file.txt

tail file.txt
```

4.	**Editing Files**

•	Use editors like vim:

```
vim file.txt
i
hello
ESC
:wq
ENTER
```
Using vim, we can copy and paste text into our text.file, and save it.
```
less file.txt
```

 ## Section 2: Using an HPC Cluster**

HPC clusters are shared resources with multiple users and nodes for heavy computations. Here’s how to interact with them.

**Understanding HPC Components**

•	**Login Node**: The entry point where you manage files and submit jobs.

•	**Compute Node**: Where your jobs are executed.

•	**Scheduler**: Manages job queues (e.g., SLURM, PBS, or LSF).

**Job Submission with SLURM**

SLURM (Simple Linux Utility for Resource Management) is a common scheduler. You interact with SLURM using scripts and commands.

1.	**Writing a Job Script**

Example job script (job_script.sh):

```bash
#!/bin/bash

#SBATCH --job-name=genome_analysis

#SBATCH --output=output.log

#SBATCH --error=error.log

#SBATCH --time=24:00:00

#SBATCH --nodes=1

#SBATCH --ntasks=4

#SBATCH --mem=16G

module load bioinformatics_tool

tool_command input_file output_file

```

2.	**Submitting a Job**

Use sbatch to submit:

```bash
sbatch job_script.sh
```

3.	**Monitoring Jobs**

•	squeue – Check the status of jobs:

```bash
squeue -u your_username
```

•	scancel – Cancel a job:

```bash
scancel job_id
```

4.	**Checking Job Output**

After your job finishes, check the output files specified in the script (output.log, error.log).

**Section 3: Practical Exercises**

1.	**Navigation Challenge**:

Navigate to a specified directory, create a new directory, and copy files into it.

2.	**Simple Job Submission**:

Write a SLURM script that prints “Hello, HPC!” to an output file, submit it, and check the result.

3.	**Data Transfer**:

Transfer a dataset from your local machine to the HPC and verify the transfer.

4.	**Genome Analysis**:

Run a simple genome analysis command using a bioinformatics tool like bcftools or samtools.

**Section 4: Best Practices**

1.	**Use Descriptive Names**: For directories, files, and job scripts.

2.	**Check Resource Usage**: Avoid over-requesting memory or CPUs.

3.	**Automate Repetitive Tasks**: Use Bash loops or scripts.

4.	**Keep Logs**: Redirect output and error streams for troubleshooting.

5.	**Clean Up**: Remove intermediate files after use to save storage.

**Resources**

•	[Linux Command Cheat Sheet](https://www.linuxcommand.org/)

•	[SLURM Documentation](https://slurm.schedmd.com/documentation.html)

•	[Bioinformatics Tools Documentation](https://bioinformatics.org/tools/)

By the end of this tutorial, you should be comfortable navigating a Linux environment and submitting jobs to an HPC cluster. These skills are foundational for performing WGS analyses in conservation genomics. Let’s dive in!
