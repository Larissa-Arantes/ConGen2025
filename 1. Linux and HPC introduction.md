***Introduction to Linux and HPC for Conservation Genomics***

Welcome to the Linux and HPC tutorial for conservation genomics! In this session, we’ll focus on the essential skills needed to navigate the command line, manage files, and run analyses on a high-performance computing (HPC) cluster. This tutorial is designed for graduate students with varying levels of experience, so don’t worry if you’re new to Linux or HPC environments—we’ll take it step by step.

**Why Linux and HPC for Conservation Genomics?**

Whole-genome sequencing (WGS) generates large datasets that require powerful computational resources for analysis. HPC clusters provide the necessary infrastructure, but using them effectively requires familiarity with Linux, the command-line interface (CLI), and job scheduling systems.

**Section 1: Basics of the Linux Command Line**

The Linux terminal is your gateway to controlling the HPC environment. Here are the basics to get you started:

**Accessing the Terminal**

Go to the website: https://ondemand.anvil.rcac.purdue.edu

**Navigation and File Management**

1.	**Navigating Directories**

•	pwd – Print your current directory:

```bash
pwd
```

•	ls – List files in the current directory:

```bash
ls
```

•	cd – Change directory:

```bash
cd /path/to/directory
```

•	Use cd .. to move up one directory.

2.	**Creating and Managing Files/Directories**

•	mkdir – Create a directory:

```bash
mkdir new_directory
```

•	touch – Create an empty file:

touch file.txt

•	cp – Copy files:

```bash
cp file.txt /destination/path/
```

•	mv – Move or rename files:

```bash
mv file.txt new_name.txt
```

•	rm – Remove files:

```bash
rm file.txt
```

•	Use rm -r to remove directories.

3.	**Viewing File Contents**

•	cat – View the entire file:

```bash
cat file.txt
```

•	less – View large files page by page:

```bash
less file.txt
```

•	head/tail – View the first/last lines of a file:

```bash
head file.txt

tail file.txt
```

4.	**Editing Files**

•	Use editors like nano, vi, or vim:

```bash
nano file.txt
```

**Section 2: Using an HPC Cluster**

HPC clusters are shared resources with multiple users and nodes for heavy computations. Here’s how to interact with them.

**Understanding HPC Components**

•	**Login Node**: The entry point where you manage files and submit jobs.

•	**Compute Node**: Where your jobs are executed.

•	**Scheduler**: Manages job queues (e.g., SLURM, PBS, or LSF).

**Transferring Files to/from the HPC**

1.	Use scp to copy files:

```bash
scp local_file.txt your_username@hpc.server.address:/path/to/destination/
```

2.	Use rsync for efficient transfers:

```bash
rsync -av local_directory/ your_username@hpc.server.address:/remote/directory/
```

**Job Submission with SLURM**

SLURM (Simple Linux Utility for Resource Management) is a common scheduler. You interact with SLURM using scripts and commands.

1.	**Writing a Job Script**

Example job script (job_script.sh):

```bash
#!/bin/bash

#SBATCH --job-name=genome_analysis

#SBATCH --output=output.log

#SBATCH --error=error.log

#SBATCH --time=24:00:00

#SBATCH --nodes=1

#SBATCH --ntasks=4

#SBATCH --mem=16G

module load bioinformatics_tool

tool_command input_file output_file

```

2.	**Submitting a Job**

Use sbatch to submit:

```bash
sbatch job_script.sh
```

3.	**Monitoring Jobs**

•	squeue – Check the status of jobs:

```bash
squeue -u your_username
```

•	scancel – Cancel a job:

```bash
scancel job_id
```

4.	**Checking Job Output**

After your job finishes, check the output files specified in the script (output.log, error.log).

**Section 3: Practical Exercises**

1.	**Navigation Challenge**:

Navigate to a specified directory, create a new directory, and copy files into it.

2.	**Simple Job Submission**:

Write a SLURM script that prints “Hello, HPC!” to an output file, submit it, and check the result.

3.	**Data Transfer**:

Transfer a dataset from your local machine to the HPC and verify the transfer.

4.	**Genome Analysis**:

Run a simple genome analysis command using a bioinformatics tool like bcftools or samtools.

**Section 4: Best Practices**

1.	**Use Descriptive Names**: For directories, files, and job scripts.

2.	**Check Resource Usage**: Avoid over-requesting memory or CPUs.

3.	**Automate Repetitive Tasks**: Use Bash loops or scripts.

4.	**Keep Logs**: Redirect output and error streams for troubleshooting.

5.	**Clean Up**: Remove intermediate files after use to save storage.

**Resources**

•	[Linux Command Cheat Sheet](https://www.linuxcommand.org/)

•	[SLURM Documentation](https://slurm.schedmd.com/documentation.html)

•	[Bioinformatics Tools Documentation](https://bioinformatics.org/tools/)

By the end of this tutorial, you should be comfortable navigating a Linux environment and submitting jobs to an HPC cluster. These skills are foundational for performing WGS analyses in conservation genomics. Let’s dive in!
