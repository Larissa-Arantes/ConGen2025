# Introduction to Linux and HPC for Conservation Genomics

Welcome to the Linux and HPC tutorial for conservation genomics! In this session, we’ll focus on the essential skills needed to navigate the command line, manage files, and run analyses on a high-performance computing (HPC) cluster.

**Why Linux and HPC for Conservation Genomics?

Whole-genome sequencing (WGS) generates large datasets that require powerful computational resources for analysis. HPC clusters provide the necessary infrastructure, but using them effectively requires familiarity with Linux, the command-line interface (CLI), and job scheduling systems.

## Section 1: Basics of the Linux Command Line

The Linux terminal is your gateway to controlling the HPC environment. Here are the basics to get you started:

### Logging into Purdue Anvil

Go to the website: https://ondemand.anvil.rcac.purdue.edu

Log in with your ACCESS login name and password

Navigate to Shell Access -> now you are on the command-line interface! 

## Navigation and File Management

This is called the shell, which is a computer program that allows you to control your computer using keyboard commands rather than using your mouse. Usually we only run commands like navigating directories, making a file, or deleting files on the command line. 

- Files are organized into directories, similar to folder structure on your laptop.
- The dollar sign is the "prompt", which you type commands on the command line to navigate the directory. We will get to running scripts later.
  
1.	**Navigating Directories**

•	pwd – Print your current directory:

```
pwd
```
When we log in, we are in our home directory: `/home/x-lhennelly`

We have other directories too. I will show the different directories on my computer and draw them on the board. 
- Scratch -> used to store large data files and output results from analyses.
```
cd /anvil/scratch/x-YOURNAME/
ls 
```

- Home directory -> only used for storing you scripts, our out and error files, and small output files that are small in size
- Shared data for class -> all students have access to this directory, where we have the elephant data shared to run various analyses
```
cd /anvil/projects/x-bio240351/shared_data/
cd 
```
•	ls – List files in the current directory:

```
ls
```

•	mkdir – Create a directory:

```
mkdir 01_linux_introduction 02_sequencing_dataquality 03_readmapping_variantcalling 04_population_structure  05_genetic_diversity 06_runs_of_homozygosity 07_deleterious_variants 08_demographic_inference 09_phylogenomic_introgression
cd /anvil/scratch/x-YOURNAME/
mkdir log
ls 
```
Now you should see the directories for different days of the course

•	cd – Change directory:

```
cd 01_linux_introduction
pwd
```
Now we are in the 01_linux_introduction directory

•	Use cd .. to move up one directory.
```
cd ..
```
We can also just use `cd` and that will place us into our home directory
```
cd 01_linux_introduction
cd
```

## Making and managing files

•	touch – Create an empty file:
```
cd 01_linux_introduction
touch file.txt
```

•	cp – Copy files:

```bash
cp file.txt /home/x-lhennelly
cd ..
ls
```

•	rm – Remove files:

```bash
rm /home/x-lhennelly/file.txt
```

3.	**Viewing File Contents**

•	cat – View the entire file:

```bash
less file.txt
```

•	head/tail – View the first/last lines of a file:

```bash
head file.txt

tail file.txt
```

4.	**Editing Files**

•	Use editors like vim:

```
vim file.txt
i
hello
ESC
:wq
ENTER
```
Using vim, we can copy and paste text into our text.file, and save it. Let's look at the file. 
```
less file.txt
```
Let's look at another file to practice some basic commands
```
cd /anvil/projects/x-bio240351/Day1
ls
less country.txt
less elephantlist.txt
```
We can paste the files together: 
```
paste elephantlist.txt country.txt > elephant_country.txt
less elephant_country.txt
```
We can use head and tail to look at first and last couple of lines: 
```
head elephant_country.txt
tail elephant_country.txt
```
We can use grep to find specific texts in the file and paste into a new file
```
grep Kenya elephant_country.txt > elephant_Kenya.txt
less elephant_Kenya.txt
```
We can also choose specific lines and columns in a file and paste into a new file: 
```
sed -n 15p elephant_country.txt
awk '{print $2}' elephant_country.txt 
```

## SCAVENGER HUNT
Let's practice changing directories and adjusting files with a small game. You will use the commands below to grab a specific letter for each command, and you will find out what word all the letters will spell.  
```
cd /anvil/projects/x-bio240351/Day1/Game
less file1.txt
sed -n 9p file1.txt
less file2.txt
awk '{print $5}' file2.txt
tail -n 1 file3.txt
head -n 1 file3.txt
grep "o" file2.txt 
awk '/next/ {print}' file4.txt
grep -v "d" file6.txt
```
What is the final word? 

 ## Section 2: Using an HPC Cluster**

HPC clusters are shared resources with multiple users and nodes for heavy computations. Here’s how to interact with them.

**Understanding HPC Components**

•	**Login Node**: The entry point where you manage files and submit jobs.

•	**Compute Node**: Where your jobs are executed.

•	**Scheduler**: Manages job queues (e.g., SLURM, PBS, or LSF).

**Job Submission with SLURM**

SLURM (Simple Linux Utility for Resource Management) is a common scheduler. You interact with SLURM using scripts and commands.

1.	**Writing a Job Script**

First we will make the job script: 
```
touch 01_testscript.sh
```

Then we will copy and paste the text below into the  `01_testscript.sh` 
```
vim 01_testscript.sh
i
copy and paste exmample job script below
ESC
:wq
```
Example job script (job_script.sh):

```
#!/bin/bash
#SBATCH --job-name testscript
#SBATCH -A bio240351  # Allocation name
#SBATCH --nodes=1         # Total # of nodes (must be 1 for serial job)
#SBATCH --ntasks=1        # Total # of MPI tasks (should be 1 for serial job)
#SBATCH --time=1:30:00    # Total run time limit (hh:mm:ss)
#SBATCH -o /home/YOUR_USERNAME/logs/testscript.o%j      # Name of stdout output file
#SBATCH -e /home/YOUR_USERNAME/logs/testscript.e%j      # Name of stderr error file
#SBATCH -p wholenode  # Queue (partition) name




```

2.	**Submitting a Job**

Use sbatch to submit:

```bash
sbatch job_script.sh
```

3.	**Monitoring Jobs**

•	squeue – Check the status of jobs:

```bash
squeue -u your_username
```

•	scancel – Cancel a job:

```bash
scancel job_id
```

4.	**Checking Job Output**

After your job finishes, check the output files specified in the script (output.log, error.log).

**Section 3: Practical Exercises**

1.	**Navigation Challenge**:

Navigate to a specified directory, create a new directory, and copy files into it.

2.	**Simple Job Submission**:

Write a SLURM script that prints “Hello, HPC!” to an output file, submit it, and check the result.

3.	**Data Transfer**:

Transfer a dataset from your local machine to the HPC and verify the transfer.

4.	**Genome Analysis**:

Run a simple genome analysis command using a bioinformatics tool like bcftools or samtools.

**Section 4: Best Practices**

1.	**Use Descriptive Names**: For directories, files, and job scripts.

2.	**Check Resource Usage**: Avoid over-requesting memory or CPUs.

3.	**Automate Repetitive Tasks**: Use Bash loops or scripts.

4.	**Keep Logs**: Redirect output and error streams for troubleshooting.

5.	**Clean Up**: Remove intermediate files after use to save storage.

**Resources**

•	[Linux Command Cheat Sheet](https://www.linuxcommand.org/)

•	[SLURM Documentation](https://slurm.schedmd.com/documentation.html)

•	[Bioinformatics Tools Documentation](https://bioinformatics.org/tools/)

By the end of this tutorial, you should be comfortable navigating a Linux environment and submitting jobs to an HPC cluster. These skills are foundational for performing WGS analyses in conservation genomics. Let’s dive in!
